{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK's corpora: An introduction\n",
    "=======\n",
    "\n",
    "_Practical Python for Linguistics and the Humanities -- Alexis\n",
    "Dimitriadis_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "TableOfContents": 1
   },
   "source": [
    "## Contents\n",
    "\n",
    "\n",
    "**[1. Preliminaries: Corpora and other \"data\"](#1.-Preliminaries:-Corpora-and-other-\"data\")**  \n",
    "\n",
    "**[2. Corpora in the NLTK](#2.-Corpora-in-the-NLTK)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.1 Recap: How we open a regular file](#2.1-Recap:-How-we-open-a-regular-file)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.2 How we access NLTK corpora](#2.2-How-we-access-NLTK-corpora)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.3 Reading words from a corpus](#2.3-Reading-words-from-a-corpus)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.4 Reading sentences from a corpus](#2.4-Reading-sentences-from-a-corpus)  \n",
    "\n",
    "**[3. Some common tasks](#3.-Some-common-tasks)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [3.1 Examining each sentence in a corpus](#3.1-Examining-each-sentence-in-a-corpus)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [3.2 Making a list of words into a string again](#3.2-Making-a-list-of-words-into-a-string-again)  \n",
    "\n",
    "**[4. The common NLTK corpus model](#4.-The-common-NLTK-corpus-model)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.1 Categorized corpora](#4.1-Categorized-corpora)  \n",
    "\n",
    "**[5. Activities: Working with NLTK corpora](#5.-Activities:-Working-with-NLTK-corpora)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.1 Exercise A1](#5.1-Exercise-A1)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.2 Exercise A2](#5.2-Exercise-A2)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.3 Exercise A3](#5.3-Exercise-A3)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.4 Exercise A4](#5.4-Exercise-A4)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.5 Exercise A5](#5.5-Exercise-A5)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.6 Exercise A6](#5.6-Exercise-A6)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.7 Exercise A7](#5.7-Exercise-A7)  \n",
    "\n",
    "**[6. What we learned](#6.-What-we-learned)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [6.1 What you should know by heart](#6.1-What-you-should-know-by-heart)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [6.2 What you should remember you saw](#6.2-What-you-should-remember-you-saw)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK) is a collection of programs and\n",
    "resources for teaching and carrying out tasks in natural language\n",
    "processing. Today we focus on corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###           Background reading\n",
    "\n",
    "[Chapter 2, section 1][ch2.1] of the NLTK book, for the NLTK corpus\n",
    "environment\n",
    "\n",
    "[ch2.1]: http://www.nltk.org/book/ch02.html#accessing-text-corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries: Corpora and other \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK **software** is included as part of Anaconda, but its collection of **corpora** and other \"NLTK data\" must be downloaded separately.\n",
    "In the PC classrooms, we have taken care of that for you; at home or\n",
    "in the Mac rooms, download a starter kit yourself by running the\n",
    "following commands. They will download the collection of resources\n",
    "discussed in the NLTK book and make them available for later use. A\n",
    "resource only needs to be downloaded **once** (per computer), hence this\n",
    "code tests the waters and only launches the download if the NLTK\n",
    "resources are absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26304/1664386529.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ok\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The nltk is ready to use\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2464\u001b[0m \u001b[1;31m# Aliases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2465\u001b[1;33m \u001b[0m_downloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2466\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;31m# decide where we're going to save things to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[1;31m# variety of system-wide locations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.pos_tag(\"ok\".split())\n",
    "    print(\"The nltk is ready to use\")\n",
    "except LookupError:\n",
    "    print(\"Downloading the nltk's \\\"book\\\" bundle. This will take a few minutes.\")\n",
    "    nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you call `nltk.download()` without an argument, it will pop up an\n",
    "interactive graphical window that allows you to browse the NLTK's\n",
    "collection of resources. You can use it to download individual corpora\n",
    "and resources, or bundled \"collections\" like `\"book\"`. \n",
    "\n",
    "On some configurations, this window cannot be opened from inside a Notebook\n",
    "without special arrangements.  If necessary, run it from the IPython console in Spyder, from a script run with Spyder, or from a command-line python\n",
    "prompt. \n",
    "But first check if the downloader window is simply hidden under other windows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a resource is successfully installed, you can access it through the\n",
    "NLTK modules that are designed to use it. After installing the book\n",
    "bundle, you will (inter alia) be able to execute the following test\n",
    "code successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "\n",
    "1. If you have not done so yet, run the code cell above that installs\n",
    "the NLTK's \"book\" bundle. Check that it succeeded by running the test\n",
    "snippet.<p/>\n",
    "\n",
    "* Pop up an interactive `nltk.download()` window. Browse the available\n",
    "resources, and find and install the Alpino parsed corpus of Dutch.\n",
    "<p/>\n",
    "\n",
    "* To confirm that the Alpino corpus is now available, import the\n",
    "object `alpino` from `nltk.corpus` and view its README text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpora in the NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK gives us easy access to its collection of corpora (and later,\n",
    "to our own).\n",
    "\n",
    "Corpora usually consist of a lot of files. We've learned how to open\n",
    "one or more files directly with `os.listdir()` and `open()`, and how\n",
    "to read their contents in various ways. But the NLTK provides ways of\n",
    "accessing an entire corpus with one command, and different formats for\n",
    "its text. In hopes of preventing confusion, we'll compare the two\n",
    "methods now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Recap: How we open a regular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RedCircle.txt\") as conn: # or the path to this file, e.g. \"../practicum-4/RedCircle.txt\"\n",
    "    text = conn.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open files with `open()` and read their contents with `read()`, as\n",
    "a single long **string**. We then split them into suitably-sized\n",
    "pieces (words or lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string method `split()`, gives us a **list of words** (with\n",
    "punctuation still attached):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one',\n",
    "... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the string method `splitlines()`, we split our text into a\n",
    "**list of lines:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['of existence; and had lived nearly twenty-one years in',\n",
    "'the world with very little to distress or vex her.',\n",
    "'',\n",
    "...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "\n",
    "Split the string `text` into a list of lines and into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's easy to work with lines, \n",
    "actually we're almost always interested in **sentences**, not lines. \n",
    "And while we have been splitting strings on whitespace, it's better to\n",
    "separate punctuation from words so that the word tokens are easier to\n",
    "work with.\n",
    "The NLTK can do these things for us; but it has a _different_ way of\n",
    "representing text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How we access NLTK corpora\n",
    "\n",
    "As we have already seen, standard NLTK corpora are simply imported\n",
    "from the `nltk.corpus` module. Here is another one, a selection of\n",
    "texts from the Gutenberg project. It is part of the NLTK's \"book\"\n",
    "collection, so it should now be present and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `gutenberg` is not a list of words or sentences: It is a\n",
    "\"corpus reader\" object that knows where to find the Gutenberg corpus\n",
    "data (which the NLTK downloader installed in a known location). The\n",
    "corpus reader's methods allow us to access its contents in various\n",
    "ways without specifying a path.\n",
    "\n",
    "Among other things we can list the files that make up the corpus, or\n",
    "we can get a list of the words or _sentences_ of selected files, or of\n",
    "the entire corpus; all **without explicitly opening a single file.**\n",
    "Here's how to list the files that make up the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fileids()` method returns an ordinary list of strings. We can save the list in a variable, loop over it to print the file names one per line, etc. We won't need to open the files ourselves, though. The `nltk` does that for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reading words from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a list of all word tokens in a corpus is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the words from just one file from the corpus, specify it as an\n",
    "argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense = gutenberg.words(\"austen-sense.txt\")\n",
    "print(sense[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more readably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(sense[0:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify **a list of file ids**, instead of just\n",
    "one.\n",
    "```python\n",
    "sometexts = gutenberg.words([\"austen-emma.txt\", \"blake-poems.txt\"])\n",
    "```\n",
    "If you look carefully, you'll see that commas and other punctuation\n",
    "are returned as separate tokens: The NLTK has separated them from the\n",
    "word they were attached to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often there's no need to assign the words to a variable, since we can\n",
    "process the value returned by `words()` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in gutenberg.words(\"austen-sense.txt\"):\n",
    "    if w.endswith(\"inging\"):\n",
    "        print(w, end=\" \")\n",
    "        # or do something else with the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "\n",
    "Print out all words in Jane Austen's _Sense and Sensibility_ that end\n",
    "with `-nesses`. [There are just two.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reading sentences from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to examine our corpus one sentence at a time. The NLTK\n",
    "can give us a whole novel (or an entire corpus) as a **list of\n",
    "sentences**, with each sentence broken up into words (or more\n",
    "precisely, into tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = gutenberg.sents(fileids=\"austen-emma.txt\")\n",
    "for sent in emma[:6]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note carefully:** Each sentence is now a **list of words**, not a\n",
    "single string! A list of sentences, such as `emma`, is no longer a\n",
    "list of strings but **a list of lists of strings** (where each string\n",
    "is a word, not a sentence.)\n",
    "\n",
    "\n",
    "This is different from how we worked until now. Lists of lists are\n",
    "more complex to work with, but generally more useful.\n",
    "\n",
    "In fact, `sents()` and `words()` do not return a list, but a \"view\". The view object will read the corpus in small pieces, and only when necessary. This allows very large corpora to be searched without running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gutenberg.sents(fileids=\"austen-emma.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examining each sentence in a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching in a list of words is fine if we're just looking for\n",
    "individual words. But to see the sentences that contain some word, we\n",
    "must loop over sentences and examine each word of the current the\n",
    "sentence. If we find a match, we can print or save the entire\n",
    "sentence.\n",
    "\n",
    "Some care is required:  If what we're searching for occurs in a\n",
    "sentence twice, we don't want to count or save the sentence twice. As\n",
    "soon as we find something in a sentence, we should print or process it\n",
    "and immediately move on to the next sentence.\n",
    "\n",
    "For this we can use the `break` statement, which immediately ends\n",
    "execution of the nearest *loop.* Execution continues with the next\n",
    "statement, which might be inside another, higher loop-- as in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = gutenberg.sents('austen-emma.txt')\n",
    "examples = list()\n",
    "for sentence in emma:\n",
    "    for word in sentence:\n",
    "        if word.lower() == 'these':\n",
    "            examples.append(sentence)\n",
    "            break   # \"break\" out of the nearest loop: \"for word in sentence\"\n",
    "    \n",
    "print(len(examples), 'sentences contain the word \"these\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `emma` is a **list of sentences,** and each sentence is\n",
    "a **list of words.** If it's still not clear, print out some elements\n",
    "(e.g., `emma[6]`) and look at them carefully.\n",
    "\n",
    "1. The variable `emma` is a list of sentences. We can loop over it.\n",
    "2. Each sentence is a list of words. We can loop over it.\n",
    "3. Each word is a string, and we can use string methods or regexp\n",
    "searches on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Making a list of words into a string again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to join the list of words back into a single string,\n",
    "e.g. to easily print a sentence in a readable way, or to save it to a\n",
    "file with `write()`. Recall also that regular expressions can only\n",
    "search in a string, not in a list of words. To transform *each\n",
    "sentence* into a separate string, we loop over the list of sentences\n",
    "and transform each one into a string using `join()`. The punctuation\n",
    "looks a little odd, but it's readable enough and easy to search with a regexp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in emma[55:60]:\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "\n",
    "Print (readably, as shown above) only the sentences from Jane Austen's\n",
    "_Emma_ that contain the word `contemptible`. Repeat with the word\n",
    "\"abominable\". (Note how they differ in frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The common NLTK corpus model\n",
    "\n",
    "NLTK corpora have a consistent interface. E.g., they all have a\n",
    "`fileids()` method. If the corpus has a `README` file with information, we can view it with the `readme()` method. \n",
    "All corpora have methods that make the text available in\n",
    "a choice of formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg.raw()    # The corpus as one huge string\n",
    "gutenberg.words()  # All words (tokens) as a flat list of strings\n",
    "gutenberg.sents()  # A sent is a list of words\n",
    "gutenberg.paras()  # A paragraph is a list of sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these can be restricted to one or\n",
    "more files by specifying the file ids. Multiple file ids must be given\n",
    "as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "all_austen = gutenberg.paras(['austen-emma.txt', \n",
    "                              'austen-persuasion.txt', 'austen-sense.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many corpora are subdivided into categories; others include annotations to the text, such as part of speech\n",
    "tags (POS tags). These features are available through additional methods, which are also consistently named. We'll see some of them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Categorized corpora\n",
    "\n",
    "The Brown corpus consists of hundreds of files, which are \"categorized\" into a number of different genres (types of text). We can list them with the method `categories()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories, like file ids, are returned in an ordinary list which we\n",
    "can use with other python functions. When listing file IDs or\n",
    "extracting text, we can use the argument `categories` to restrict\n",
    "results to files belonging to one or more categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.fileids(categories='government'))\n",
    "\n",
    "newstexts = brown.sents(categories='news')\n",
    "print(len(newstexts), 'sentences in the \"news\" category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many corpora are not subdivided into categories, and have no `categories()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activities: Working with NLTK corpora\n",
    "\n",
    "**Most of the activities rely on the Gutenberg corpus,** which\n",
    "includes several novels by Jane Austen.\n",
    "\n",
    "Starred exercises are more challenging than unstarred ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Exercise A1\n",
    "\n",
    "Import the nltk's Brown corpus, as follows:\n",
    "\n",
    "    \n",
    "```python\n",
    "from nltk.corpus import brown\n",
    "```\n",
    "a. Find out *how many* files it contains. (Do **not** count them by\n",
    "hand!)\n",
    "\n",
    "b. Report the total number of files, words and sentences in the Brown\n",
    "   corpus. It's easier to just count punctuation as \"words\" too (i.e. if you just count tokens), but you\n",
    "   can choose to filter it out and only count words that contain\n",
    "   letters or numbers.\n",
    "\n",
    "   Hint: The value returned by `words()` etc. is a kind of list\n",
    "(really a \"view\"),\n",
    "   so you can use `len()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Exercise A2\n",
    "\n",
    "Find and print all words in Jane Austen's \"Emma\" that\n",
    "start with \"ung\" or \"Ung\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Exercise A3\n",
    "\n",
    "Instead of printing the words from the previous\n",
    "problem,\n",
    "   collect them into a list.\n",
    "   Print the number of words you found (with an explanatory message)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Exercise A4\n",
    "\n",
    "Construct a *set* of all words ending in -ings from\n",
    "_all_ of Jane Austen's novels. How many different words were found?\n",
    "Make your code report the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Exercise A5\n",
    "**Count** how many words in the gutenberg corpus end\n",
    "in -ings, and how many sentences contain one or more such words. There\n",
    "are 4717 words in 4133 sentences, so don’t print them all out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Exercise A6\n",
    "\n",
    "Count and report how many words in the gutenberg\n",
    "corpus contain two k’s, not necessarily adjacent. Don’t forget about\n",
    "capitalization. [Correct answer: 355]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Exercise A7\n",
    "\n",
    "Collect into a list all __sentences__ from Jane\n",
    "Austen's novels that contain a word with two k's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What we learned\n",
    "\n",
    "### 6.1 What you should know by heart\n",
    "\n",
    "* How to gain access to the nltk's `brown` and `gutenberg` corpora.\n",
    "* How to iterate over the words and sentences in an nltk corpus.\n",
    "* How to print out a sentence (or several) in a readable way.\n",
    "* How to search over all sentences, or over all words, in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 What you should remember you saw\n",
    "\n",
    "* The use of the methods `fileids()` and `categories()`\n",
    "* Searches can be restricted to one, or several, categories of a categorized corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
