{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9# Evaluation: NER, Chunking, and other retrieval tasks\n",
    "\n",
    "_Practical Python for Linguistics and the Humanities -- Alexis \n",
    "Dimitriadis_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "TableOfContents": 1
   },
   "source": [
    "## Contents\n",
    "\n",
    "\n",
    "**[1. Evaluating a retrieval task: Adjective recognition](#1.-Evaluating-a-retrieval-task:-Adjective-recognition)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [1.1 Prepare the testing data](#1.1-Prepare-the-testing-data)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [1.2 Examine the data](#1.2-Examine-the-data)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [1.3 Prepare the tagger](#1.3-Prepare-the-tagger)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [1.4 Tag and evaluate](#1.4-Tag-and-evaluate)  \n",
    "\n",
    "**[2. Chunking the CONLL2002 corpus](#2.-Chunking-the-CONLL2002-corpus)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.1 IOB and Tree formats](#2.1-IOB-and-Tree-formats)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [2.2 Converting between formats](#2.2-Converting-between-formats)  \n",
    "\n",
    "**[3. Evaluating the NLTK's own Named Entity chunker](#3.-Evaluating-the-NLTK's-own-Named-Entity-chunker)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [3.1 Error analysis](#3.1-Error-analysis)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the performance of our tools is an important part of Natural\n",
    " Language Processing. While in tasks like POS tagging every word must \n",
    "be tagged, information retrieval tasks are only concerned with some \n",
    "items out of the mass of data.  The notions of precision and recall, \n",
    "and the \"F-score\" that combines them, measure the success of our \n",
    "solutions. \n",
    "\n",
    "\n",
    "## Reading:\n",
    "\n",
    "* NLTK book, [section 6.3: \n",
    "Evaluation](http://www.nltk.org/book/ch06.html#evaluation).  \n",
    "See also Jurafsky and Martin section 5.7.\n",
    "\n",
    "\n",
    "* NLTK book, [section 7.2: Chunking.](http://www.nltk.org/book/ch07.html#sec-chunking)\n",
    "\n",
    "- - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating a retrieval task: Adjective recognition\n",
    "\n",
    "We begin by evaluating a simple (and easy) information retrieval task:\n",
    " Finding adjectives\n",
    "in the Dutch [CONLL 2002 \n",
    "corpus](http://www.cnts.ua.ac.be/conll2002/ner/). We will use the \n",
    "Dutch tagger you built in a previous assignment, and evaluate its \n",
    "performance for the adjective retrieval task.\n",
    "\n",
    "The tags that come with the corpus are our \"gold standard\": We will \n",
    "assume (contrary to fact!) that they are always right. We will compare\n",
    " our tagger's performance on the test data with the \"gold standard\" \n",
    "answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare the testing data\n",
    "\n",
    "\n",
    "Once again we'll use the [CONLL 2002 \n",
    "corpus](http://www.cnts.ua.ac.be/conll2002/ner/), which comes as part \n",
    "of the NLTK. Recall that the CONLL2002 corpus includes texts in Dutch \n",
    "and in Spanish, so we must _always_ take care to only fetch the \n",
    "appropriate file. We will use the data in the file `\"ned.train\"`, with\n",
    " the same division into training and testing sentences as before: \n",
    "Sentences with index less than 1500 are for testing, and the rest are \n",
    "for training. Never test a solution on the data it was \n",
    "trained with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002 as conll\n",
    "\n",
    "allsents = conll.tagged_sents(\"ned.train\")\n",
    "test_sents = allsents[:1500]\n",
    "train_sents = allsents[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** Create an untagged version of the test part of the \n",
    "corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN:\n",
    "\n",
    "test_sents_untagged = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examine the data\n",
    "\n",
    "Find out how many adjectives there _really_ are in the **testing \n",
    "part** of the corpus. You'll need to loop over the sentences in \n",
    "`test_sents` and count how many words are tagged as adjectives. Print \n",
    "out the result nicely. There are over one thousand adjectives in the \n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the tagger\n",
    "\n",
    "1. Use your notebook \"training a tagger\" to create and pickle a Dutch \n",
    "tagger, trained on the data in `train_sents`. If you still have your \n",
    "pickled tagger, you may use it instead of training a new one.<p/>\n",
    "\n",
    "2. In the code cell below, load your pickled tagger into a variable \n",
    "`tagger`. (Move or copy your tagger to the directory containing this notebook, \n",
    "if necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tag and evaluate\n",
    "\n",
    "Tag the untagged version of our test set with the tagger and save the \n",
    "result in a list. Compare the newly tagged sentences with the \n",
    "correctly tagged \"gold standard\": Count the number of adjectives that \n",
    "are correctly retrived, that were missed, and that are false \n",
    "positives. (I.e., true positives, false negatives, and false \n",
    "positives.) Print these numbers out for our information.\n",
    "\n",
    "Hint: To iterate over two lists of sentences in parallel, you can use \n",
    "either an index variable or the function `zip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN:\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "print(\"Found correctly:\", found)\n",
    "print(\"Missed:         \", missed),\n",
    "print(\"False positives:\", falsepos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print out the precision, recall, and F-score that you \n",
    "calculate from them. (Definitions in the NLTK book's section on \n",
    "[evaluation](http://www.nltk.org/book/ch06.html#evaluation).)\n",
    "\n",
    "If your tagger was built according to our recipe (with all the \n",
    "back-off taggers in place),  recall should be around 0.82 and \n",
    "precision around 0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN:\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "print(\"Recall:    %.3f\" % recall)\n",
    "print(\"Precision: %.3f\" % precision)\n",
    "print(\"F-score:   %.3f\" % f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking the CONLL2002 corpus\n",
    "\n",
    "\n",
    "CONLL stands for _Computational Natural Language Learning_, and each \n",
    "year's version is targeted to a different NLP task. CONLL2002 is about\n",
    " \"language independent\" Named Entity regognition, by which they meant \n",
    "languages other than English: Dutch and Spanish data are included.\n",
    "\n",
    "The CONLL2002 corpus is loaded in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002 \n",
    "conll = conll2002 # Save us some typing with this alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus provides the usual methods `words(), sents(), \n",
    "tagged_words(), tagged_sents(), fileids()`, as well as some special\n",
    "ones: `chunked_sents()`, `iob_sents()`, and `iob_words()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 IOB and Tree formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because named entities can consist of several words, the task of \n",
    "recognizing them falls in the category of _chunking:_ Recognizing and \n",
    "categorizing small groups of words. \n",
    "As the NLTK book [explains][iob], the **IOB format** (Inside, Outside,\n",
    " Begin) is a\n",
    "commonly used form of tagging that identifies chunks. Text in the\n",
    "NLTK's IOB format looks like tagged sentences and words, except there\n",
    "are three items in each tuple: _word, POS-tag, IOB-tag._\n",
    "\n",
    "[iob]: http://www.nltk.org/book/ch07.html#representing-chunks-tags-vs-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in conll.iob_sents(\"ned.train\")[14]:\n",
    "    print(w)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** \n",
    "\n",
    "List the named entities in the above sentence, along with their type."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NAMED ENTITIES:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the NLTK's designers decided to also provide a **different,** more\n",
    " convenient format.  The\n",
    "method `chunked_sents()` returns instances of the nltk's `Tree` class,\n",
    "a very special kind of `list` with several extra methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = conll.chunked_sents(\"ned.train\")[14]\n",
    "print(\"Type:\", type(sample))\n",
    "print(\"Top-level label:\", sample.label())\n",
    "print(\"Contents:\\n\"+repr(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `repr()` above to show the internal structure of our sample tree. \n",
    "When printed the regular way, the `Tree` class displays itself in a \n",
    "nice readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your python configuration supports it (our Notebooks do), the \n",
    "method `draw()` will\n",
    "display an image of the tree structure in a separate window. \n",
    "You'll have to find the pop-up window (it might be hidden behind other windows, like the `nltk.downloader()` window was) and close it to continue execution. \n",
    "\n",
    "**Note.** If the tree window does not appear, it _may_ help to add this \"magic\" notebook command at the top of the code cell, above your code:\n",
    "\n",
    "    %matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that this tree contains branches for the two named\n",
    "entities, _Ruimtelijke Ordening_ (an organization) and _BPA_\n",
    "(\"miscellaneous\").\n",
    "\n",
    "* A chunked sentence is a Tree of maximal height 3: the sentence node,\n",
    "the chunks, and the chunk words. (The height will be 2 if there are no chunks in the sentence.)\n",
    "\n",
    "\n",
    "* If a sentence contains no chunks, it will have the usual tagged\n",
    "sentence structure: A list of `(word, pos-tag)` pairs. Although it is\n",
    "a `Tree`, not a `list`, you can use all the usual list methods.\n",
    "\n",
    "\n",
    "* Each chunk is a `Tree` containing the chunk's\n",
    "tagged words. This also applies to one-word \"chunks\" (i.e., one-word \n",
    "named entities). \n",
    "Unchunked words are direct children of the top-\n",
    "level `Tree`, as in untagged sentences.\n",
    "\n",
    "Note that the above only describes _chunked_ trees. Full sentence structure consists of many levels of trees and subtrees. The nltk's parsed corpora, for example, have deep trees with many levels of branches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tree` object is also a list. The list elements are the children of\n",
    "the top node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type:\", type(sample))\n",
    "print(\"Top-level label:\", sample.label())\n",
    "print(\"Elements:\")\n",
    "for part in sample:\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the first named entity, which is at index zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type:\", type(sample[0]))\n",
    "print(\"Chunk label:\", sample[0].label())\n",
    "for part in sample[0]:\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish between words (i.e., tuples) and subtrees, use the\n",
    "function `isinstance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree  # Import the type so we can check for it\n",
    "for part in sample:\n",
    "    if isinstance(part, Tree):\n",
    "        print(part, \"---> a subtree\")\n",
    "    else:\n",
    "        print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all tree components are either subtrees or tuples, we could have\n",
    " instead checked which parts have type `tuple`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Converting between formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers that actually find and classify the chunks work with\n",
    "the IOB format, which allows chunking to be approached as a word\n",
    "classification problem. But Tree-structured chunks are easier to work\n",
    "with in many ways, so the NLTK uses them as its default\n",
    "representation. E.g., `chunked_sents()` will give you Trees, not IOB\n",
    "tags. The NLTK provides functions for converting among the two\n",
    "formats. (Use `help()` for details, or consult the book and\n",
    "documentation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "# Convert sample tree to IOB list\n",
    "iobsample = tree2conlltags(sample)\n",
    "for wordtuple in iobsample:\n",
    "    print(wordtuple)\n",
    "    \n",
    "print()\n",
    "# Convert IOB list to tree again\n",
    "newtree = conlltags2tree(iobsample)\n",
    "print(newtree)\n",
    "\n",
    "print(\"\\nSame as we started with?\", newtree == sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating the NLTK's own Named Entity chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In brief:** We'll process the CONLL2002 corpus with the NLTK's \n",
    "built-in named entity recognizer, and measure its performance using \n",
    "the NLTK's own tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk comes with a bundled Named Entity Recognizer, available as\n",
    "`nltk.ne_chunk()` or `nltk.ne_chunk_sents()` (for one sentence or a \n",
    "list of\n",
    "sentences, respectively). Like the NLTK's POS tagger, its NE \n",
    "recognizer was trained for English.\n",
    "Can we use it on the Dutch data?\n",
    "\n",
    "There are several reasons this might not work: The kind of texts it \n",
    "was\n",
    "trained on might be too different from Dutch news text; named entities\n",
    "might look different in the two languages (e.g., Dutch names do often\n",
    "have different structure from common American names); and the POS tags\n",
    "in our text might not match those that the NER expects. (The\n",
    "recognizer needs text that has already been POS-tagged.)\n",
    "\n",
    "The shortcut `nltk.ne_chunk()` will let us process text easily, but to\n",
    "evaluate its performance it is better to load the pickled recognizer\n",
    "model that `ne_chunk()` uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "default_NER = nltk.data.load('chunkers/maxent_ne_chunker/english_ace_multiclass.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is a form of shallow parsing, so the NE recognizer's chunking methods are called \n",
    "`parse()` (for one sentence) and `parse_sents()` (for a list\n",
    "of sentences). Like other NLTK tools, it also has an `evaluate()`\n",
    "method that can be used to generate a performance report. \n",
    "\n",
    "The NLTK's evaluation methods are used with a dataset that already \n",
    "includes\n",
    "the correct answer. They remove the answer, process the data, and\n",
    "compare the result against the correct answers. The NER chunker\n",
    "evaluates \"IOB accuracy\" by checking all IOB tags (i.e., all words,\n",
    "whether or not they are part of a chunk); the other measures count how\n",
    "many NE chunks were found andd identified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** Evaluate `default_NER` with the first 5000 sentences of\n",
    " the CONLL file `\"ned.train\"`. You should get \"IOB Accuracy\" around \n",
    "90%, but zero precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = ...\n",
    "print(default_NER.evaluate(testdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Error analysis\n",
    "\n",
    "The IOB accuracy is high because most words are not in a named entity chunk, and they are correctly tagged as \"O\" (outside a chunk). But our recognizer found a negligible proportion of all named entities (0%\n",
    " recall). Of the chunks it marked as named entities, a small \n",
    "proportion (7.7%) were indeed named entities; the rest were identified\n",
    " incorrectly. But how many chunks are we talking about, and what do \n",
    "they look like? We can find out by saving the value returned by \n",
    "`evaluate()`, which is actually an object, and using its methods to \n",
    "get more information.\n",
    "\n",
    "The methods `correct()`, `guessed()`, and `missed()` and `incorrect()`\n",
    " give us not just counts, but a list of the actual chunks that fall in\n",
    " each category. There seems to be no direct way to get the chunks that\n",
    " were correctly retrieved, but the information is not hard to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = default_NER.evaluate(conll.chunked_sents(\"ned.testa\"))\n",
    "print(metrics)\n",
    "print()\n",
    "\n",
    "correct = metrics.correct()\n",
    "print(\"Actual NEs in the test data:\", len(correct))\n",
    "print()\n",
    "\n",
    "guessed = metrics.guessed()\n",
    "print(\"Chunks guessed (proposed) by the recognizer:\", len(guessed), guessed[:5], \"...\")\n",
    "print()\n",
    "\n",
    "# There seems to be no way to get this via the API\n",
    "found = [ v[1] for v in metrics._tp ]\n",
    "print(\"Found correctly (truepos):\", len(found), found)\n",
    "print()\n",
    "\n",
    "incorrect = metrics.incorrect()\n",
    "print(\"Incorrect guesses (falsepos):\", len(incorrect), incorrect[:5], \"...\")\n",
    "print()\n",
    "\n",
    "print(\"Missed NEs: (falseneg)\", len(metrics.missed()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are developing our own classifier, access to the classifier's \n",
    "mistakes is essential for _error analysis,_ which may allow us to \n",
    "identify weak spots and ways to remedy them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:**\n",
    "\n",
    "1. Print out nicely (one per line) the first 50 incorrect guesses. \n",
    "\n",
    "2. Form a set of all different IOB labels appearing in the CONLL corpus (file `ned.train`). Examine them and compare them with the labels generated by the default NE chunker. (The meaning of the non-obvious labels is documented in the NLTK book). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
